# Dataset Manager gFresh

CLI para descargar datos de inferencia, generar informes y gestionar el ciclo de vida de datasets (creaciÃ³n, fusiÃ³n, resumen y registro en S3).

## ğŸ¯ Objetivo

Automatizar selecciÃ³n y preparaciÃ³n de imÃ¡genes de producciÃ³n para entrenar y versionar modelos de clasificaciÃ³n retail.

## ğŸ—ï¸ Estructura

`dataset_manager/` contiene los mÃ³dulos internos (extracciÃ³n, anÃ¡lisis, creaciÃ³n de datasets, descarga de imÃ¡genes y reporting) expuestos vÃ­a `main.py`.

## ğŸš€ Principales Capacidades

1. Descarga y consolidaciÃ³n de datos (Elastic + MySQL) en un Excel multiâ€‘pestaÃ±a.
2. Marcado y descarga selectiva de imÃ¡genes (referencias y devices) con modo dryâ€‘run.
3. CreaciÃ³n estructurada de datasets (clase/label_id) y operaciones auxiliares: merge, summary, registro en S3.
4. GestiÃ³n de procesos aislados (cada ejecuciÃ³n crea un directorio autoâ€‘contenedor con logs, datos, imÃ¡genes y reports).

## ğŸ“¦ InstalaciÃ³n

```bash
python -m venv venv
./venv/Scripts/activate   # Windows
pip install -r requirements.txt
cp .env.example .env  # Rellenar credenciales
```

### Credenciales

1. **Variables de entorno (.env):**

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar con credenciales reales
DB_PROD_RO_HOST=your_mysql_host
DB_PROD_RO_USER=your_readonly_user
DB_PROD_RO_PASSWORD=your_readonly_password
DB_PROD_RO_DATABASE=your_database_name
```

1. **Elasticsearch:**

Crear `config/credentials/credentials_elastic_prod.json`:

```json
{
    "host": "tu-cluster.elasticsearch.com",
    "port": 9243,
    "verify_certs": true,
    "username": "tu_usuario",
    "password": "tu_password",
    "use_ssl": true
}
```

1. **AWS S3:**

Las credenciales de S3 se configuran automÃ¡ticamente desde el archivo de Elasticsearch.

## ğŸ–¥ï¸ Uso RÃ¡pido

```bash
# Activar entorno virtual
./venv/Scripts/activate  # Windows
source venv/bin/activate # Linux/Mac

# Descarga y genera Excel del deployment 130 (Ãºltimos 30 dÃ­as)
python main.py download_info 130 --days-back 30

# Crear dataset (estructura de carpetas) a partir del proceso activo
python main.py create_dataset

# Resumen del dataset
python main.py summary_dataset -d dataset_130_010825_v1

# Registrar (ZIP + S3 + CSV global)
python main.py register_dataset -d dataset_130_010825_v1

# Fusionar varios datasets
python main.py merge_datasets

# Estructura de carpetas hoja
python main.py folder_structure
```

## ğŸ“œ Comandos

| Comando | DescripciÃ³n breve |
|---------|-------------------|
| `download_info <deployment_id>` | Descarga datos + genera Excel. |
| `active-process` | Gestiona proceso activo (listar, set, clear). |
| `status` | Muestra configuraciÃ³n y paths. |
| `review_images` | Descarga imÃ¡genes marcadas (productos + devices). |
| `create_dataset` | Crea estructura de dataset desde Excel. |
| `folder_structure` | CSV con carpetas hoja e imÃ¡genes. |
| `merge_datasets` | Fusiona varios datasets. |
| `summary_dataset` | CSV resumen cruzando imÃ¡genes con datos. |
| `register_dataset` | ZIP + S3 + registro en CSV global. |

ParÃ¡metros clave (segÃºn comando): `--days-back`, `--confidence`, `--min-appearances`, `--process-name`, `--fast`, `--limit-refs`, `--images-per-ref`, `--dry-run`, `--no-filter-used`.

### Ejemplos

```bash
# Ajustando confianza y nombre de proceso
python main.py download_info 130 --days-back 45 --confidence 0.78 --process-name dataset_130_aug_v2

# Dataset modo rÃ¡pido
python main.py create_dataset --fast --limit-refs 25 --images-per-ref 12

# No filtrar transacciones ya usadas
python main.py create_dataset --no-filter-used

# Resumen y registro sin regenerar ZIP ni reemplazar S3
python main.py summary_dataset -d dataset_130_aug_v2
python main.py register_dataset -d dataset_130_aug_v2
```

## ğŸ“Š Outputs

### Directorios del Sistema

```text
output/
â”œâ”€â”€ processes/                 # Procesos independientes
â”‚   â””â”€â”€ dataset_{id}_{date}_v1/
â”‚       â”œâ”€â”€ reports/          # Archivos Excel generados
â”‚       â”œâ”€â”€ images/           # ImÃ¡genes descargadas
â”‚       â”‚   â”œâ”€â”€ productos/    # ImÃ¡genes de referencias
â”‚       â”‚   â””â”€â”€ devices/      # ImÃ¡genes por device
â”‚       â”œâ”€â”€ data/            # Datos procesados
â”‚       â”œâ”€â”€ logs/            # Logs detallados
â”‚       â””â”€â”€ process_metadata.json
â””â”€â”€ legacy/                   # Archivos antiguos (compatibilidad)
```

### Archivos Excel Generados

#### Archivo Principal: `dataset_{deployment_id}_{timestamp}.xlsx`

**PestaÃ±as principales:**

- **`Resumen`**: MÃ©tricas ejecutivas y KPIs principales
- **`Datos_Elasticsearch`**: Dataset completo con 26K+ registros
- **`References`**: 74 referencias analizadas con mÃ©tricas
- **`Devices`**: 9 devices con anÃ¡lisis individual de rendimiento
- **`Labels`**: Clasificaciones y etiquetas del modelo
- **`Model_Data`**: ConfiguraciÃ³n del modelo activo
- **`Consistencia`**: Validaciones e inconsistencias detectadas

**PestaÃ±as de seguimiento de imÃ¡genes:**

- **`revisar_imagenes_bajadas`**: Log detallado de imÃ¡genes de productos
- **`devices_imagenes_bajadas`**: Log detallado de imÃ¡genes de devices

#### Estructura de Datos en PestaÃ±as

**References (ejemplo):**

- `reference_name`: CÃ³digo de la referencia
- `label_name`: Nombre de la etiqueta asociada
- `total_transactions`: Total de transacciones
- `ok_percentage`: % de aciertos del sistema
- `model_ok_percentage`: % de aciertos del modelo
- `revisar_imagenes`: Marca para descarga de imÃ¡genes
- MÃ©tricas adicionales por top1, top2, top3

**Devices (ejemplo):**

- `device_id`: ID Ãºnico del dispositivo
- `device_name`: Nombre del dispositivo
- `total_transactions`: Transacciones procesadas
- `ok_percentage`: Rendimiento del dispositivo
- `revisar_imagenes`: Marca para descarga de imÃ¡genes
- MÃ©tricas de precisiÃ³n y errores

## âš™ï¸ ConfiguraciÃ³n (extracto)

### Variables de Entorno (.env)

```bash
# Base de datos MySQL (Solo lectura) - REQUERIDO
DB_PROD_RO_HOST=your_mysql_host
DB_PROD_RO_USER=your_readonly_user
DB_PROD_RO_PASSWORD=your_readonly_password
DB_PROD_RO_DATABASE=your_database_name
DB_PROD_RO_PORT=3306

# ConfiguraciÃ³n opcional
LOG_LEVEL=INFO
OUTPUT_BASE_DIR=./output
```

### ConfiguraciÃ³n de Descarga de ImÃ¡genes

La configuraciÃ³n se maneja automÃ¡ticamente, pero se puede personalizar:

```yaml
image_review:
  num_imagenes_revision: 5              # ImÃ¡genes por referencia/device
  tipo_transacciones: "ambas"           # correctas|incorrectas|ambas
  clear_output_folder: true             # Limpiar carpeta antes de descargar
  tipo_imagenes_bajar: "clase_y_similares"  # Tipos de productos a descargar
  s3_bucket: "grabit-data"              # Bucket de S3
  s3_region: "eu-west-2"                # RegiÃ³n de S3
```

## ğŸ”§ Flujo Resumido

1. `download_info <deployment_id>` â†’ genera proceso + Excel.
2. Marcar en Excel referencias/devices con `revisar_imagenes=si`.
3. `review_images` (dryâ€‘run opcional, luego real).
4. `create_dataset` (opcional `--fast`).
5. `summary_dataset` y/o `merge_datasets` si hace falta combinar.
6. `register_dataset` para ZIP + S3 + registro.

## ğŸ”’ Seguridad

### Credenciales Seguras

- **Variables de entorno** para DB (archivo `.env`)
- **Archivos JSON separados** para servicios complejos
- **ExclusiÃ³n completa** del control de versiones
- **Conexiones de solo lectura** para todas las fuentes

### Archivos Protegidos (en .gitignore)

```text
.env                          # Variables de entorno
config/credentials/           # Directorio de credenciales
logs/                         # Archivos de log
output/                       # Archivos de salida
venv/                         # Entorno virtual
```

## ğŸš¨ Problemas Comunes

### Problemas Comunes

**Error de credenciales:**

```bash
python main.py status  # Verificar configuraciÃ³n
```

**Proceso activo no encontrado:**

```bash
python main.py active-process  # Ver estado actual
```

**ImÃ¡genes no se descargan:**

- Verificar credenciales de S3
- Comprobar que hay referencias/devices marcados con `revisar_imagenes=si`
- Usar `--dry-run` para simular primero

**Excel estÃ¡ abierto:**

- Cerrar archivo Excel antes de ejecutar comandos
- El sistema intentarÃ¡ 3 veces y crearÃ¡ backup si es necesario

### Logs y DiagnÃ³stico

Los logs detallados se guardan en:

- `output/processes/{process_name}/logs/`
- Console output con timestamps
- Logs especÃ­ficos por comando ejecutado

## ğŸ“‹ Requisitos

- **Python 3.8+** (verificado automÃ¡ticamente)
- **Acceso a Elasticsearch** con datos de inferencia
- **Acceso de lectura a MySQL** con tablas de referencia
- **Credenciales de AWS S3** para descarga de imÃ¡genes
- **Espacio en disco** para imÃ¡genes (aprox. 10MB por device/referencia)
- **ConexiÃ³n a internet** estable

## ğŸ”„ Cambios Clave

- Renombrado `crear_dataset` â†’ `create_dataset`.
- Integrado `review_devices` en `review_images`.
- AÃ±adidos: `folder_structure`, `merge_datasets`, `summary_dataset`, `register_dataset`.
- Registro de datasets con progreso de ZIP y subida S3 opcional.
- Manejo robusto de Excel (permisos, corrupciÃ³n parcial) y columnas enriquecidas.

## ğŸ› ï¸ Desarrollo

Para desarrollo:

```bash
# Clonar repositorio
git clone <repository_url>
cd ops_gFresh_dataset_training

# Configurar entorno de desarrollo
python setup.py
.\venv\Scripts\activate

# Instalar en modo desarrollo
pip install -e .

# Ejecutar pruebas
python -m pytest tests/ -v
```

### ExtensiÃ³n

- Usar `ProcessAwareSettings` para configuraciÃ³n
- Implementar logging con `ProcessLoggerManager`
- Seguir patrÃ³n de carpetas por proceso
- Documentar cambios en README

---

---
Autor: Alberto GÃ³mez Â· VersiÃ³n 2.1.1 Â· Agosto 2025

## ğŸ—‚ï¸ PublicaciÃ³n en GitHub

```bash
git init
git add .
git commit -m "Initial cleaned version"
git branch -M main
git remote add origin git@github.com:TU_ORG/ops_gfresh_dataset_manager.git
git push -u origin main
```

Listo.
